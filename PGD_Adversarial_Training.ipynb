{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PGD_Adversarial_training.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84w6Izuss4vN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import copy\n",
        "import cv2 as cv\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning the repository containing the dataset\n",
        "!git clone https://github.com/NavoditC/Fooled_by_AI.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzE57U8btLUC",
        "outputId": "7b7988c9-16fc-491b-fbda-4e73c1cb1b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Fooled_by_AI' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data in a dataframe\n",
        "path = 'Fooled_by_AI/images/dataset.csv'\n",
        "column_names = ['images','targets']\n",
        "df = pd.read_csv(path,names=column_names)"
      ],
      "metadata": {
        "id": "06QB4ezPtLXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6dQiYrm7d96c",
        "outputId": "f6aa9fb8-f9f7-4d25-a3e4-593d0ae65ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  images  targets\n",
              "0  images_real_moon0.png        1\n",
              "1  images_real_moon1.png        1\n",
              "2  images_real_moon2.png        1\n",
              "3  images_real_moon3.png        1\n",
              "4  images_real_moon4.png        1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3091cb6b-72cb-417f-a05e-de015f03f29a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>images</th>\n",
              "      <th>targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>images_real_moon0.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>images_real_moon1.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>images_real_moon2.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>images_real_moon3.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>images_real_moon4.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3091cb6b-72cb-417f-a05e-de015f03f29a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3091cb6b-72cb-417f-a05e-de015f03f29a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3091cb6b-72cb-417f-a05e-de015f03f29a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the csv file which containing the image paths and the steering angle, throttle, brake and speed\n",
        "dataset = []\n",
        "with open('Fooled_by_AI/images/dataset.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    next(csv_reader, None) # To skip the first line which contains the column names of the dataframe\n",
        "    for line in csv_reader:\n",
        "        dataset.append(line)"
      ],
      "metadata": {
        "id": "h3bkkhA5zDR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, samples, transform):\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_samples = self.samples[index]\n",
        "        filename = batch_samples[0]\n",
        "        target = int(batch_samples[1])\n",
        "        filepath = 'Fooled_by_AI/images/' + filename.split('/')[-1]\n",
        "        image = cv.imread(filepath)\n",
        "        image = image[60:420,143:513]\n",
        "        image = cv.resize(image,(224,224))\n",
        "        image = self.transform(image)\n",
        "        return image, target\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "metadata": {
        "id": "FR_oT3Jn1OM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "        assert num_layers in [18, 34, 50, 101, 152], \"should be a a valid architecture\"\n",
        "        super(Block, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        if self.num_layers > 34:\n",
        "            self.expansion = 4\n",
        "        else:\n",
        "            self.expansion = 1\n",
        "        # ResNet50, 101, and 152 include additional layer of 1x1 kernels\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        if self.num_layers > 34:\n",
        "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        else:\n",
        "            # for ResNet18 and 34, connect input directly to (3x3) kernel (skip first (1x1))\n",
        "            self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.num_layers > 34:\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_layers, block, image_channels, num_classes):\n",
        "        assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \\\n",
        "                                                     f'to be 18, 34, 50, 101, or 152 '\n",
        "        super(ResNet, self).__init__()\n",
        "        if num_layers < 50:\n",
        "            self.expansion = 1\n",
        "        else:\n",
        "            self.expansion = 4\n",
        "        if num_layers == 18:\n",
        "            layers = [2, 2, 2, 2]\n",
        "        elif num_layers == 34 or num_layers == 50:\n",
        "            layers = [3, 4, 6, 3]\n",
        "        elif num_layers == 101:\n",
        "            layers = [3, 4, 23, 3]\n",
        "        else:\n",
        "            layers = [3, 8, 36, 3]\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNetLayers\n",
        "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
        "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
        "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
        "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        layers = []\n",
        "\n",
        "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride),\n",
        "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
        "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
        "        self.in_channels = intermediate_channels * self.expansion # 256\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def ResNet18(img_channels=3, num_classes=2):\n",
        "    return ResNet(18, Block, img_channels, num_classes)"
      ],
      "metadata": {
        "id": "WK8I4ni2vL2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing train-test split and storing the train and test data\n",
        "total_samples = len(dataset)\n",
        "train_samples = int(0.8*total_samples)\n",
        "test_samples = total_samples - train_samples\n",
        "train_test_lengths = [train_samples,test_samples]\n",
        "train_data, test_data = random_split(dataset, lengths = train_test_lengths)\n",
        "params = {'batch_size': 4, 'shuffle': True, 'num_workers': 2}\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),                            \n",
        "    #transforms.RandomCrop((28, 28), padding=2, pad_if_needed=True, fill=0, padding_mode='constant'), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: (x / 255.0))\n",
        "])\n",
        "          \n",
        "# Define the transformation to normailize the image such that the pixel intensities lie between 0 and 1\n",
        "training_set = MyDataset(train_data, transform)\n",
        "test_set = MyDataset(test_data, transform)\n",
        "\n",
        "# Define the training and test generator which is called batch-wise during training and testing respectively\n",
        "training_generator = DataLoader(training_set, **params)\n",
        "test_generator = DataLoader(test_set, **params)"
      ],
      "metadata": {
        "id": "tGHvvCfv3paF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = ResNet18(img_channels=3, num_classes=2)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "pMoQEzU2qAD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(): \n",
        "\n",
        "    count = 0\n",
        "    loss_list = []\n",
        "    epoch_list = []\n",
        "    \n",
        "\n",
        "    num_epoch = 15\n",
        "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "        epoch_list.append(epoch+1)\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss_epoch = 0.0\n",
        "        c=0\n",
        "        for i, (inputs, labels) in enumerate(training_generator):\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = inputs.float().to(device), labels.long().to(device)\n",
        "            #inputs = inputs.permute(0,3,1,2)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            loss_epoch += loss.item()\n",
        "            c+=1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f accuracy: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 2000, 100*correct/total))\n",
        "                running_loss = 0.0\n",
        "        loss_list.append(loss_epoch/c) \n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for (inputs, labels) in test_generator:\n",
        "            inputs, labels = inputs.float().to(device), labels.long().to(device)\n",
        "            #inputs = inputs.permute(0,3,1,2)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print('Accuracy of the network on the test images: %.3f %%' % (\n",
        "            100 * correct / total))\n",
        "    plt.plot(epoch_list,loss_list)\n",
        "    plt.xlabel(\"Number of epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"CNN: Loss vs Number of epochs\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  main()"
      ],
      "metadata": {
        "id": "qP4pIjEWqAGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "713d4757-6d91-4615-821b-20c63e6227d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n",
            "Accuracy of the network on the test images: 100.000 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://niko-gamulin.medium.com/resnet-implementation-with-pytorch-from-scratch-23cf3047cb93"
      ],
      "metadata": {
        "id": "oBOlRWmts9BT"
      }
    }
  ]
}